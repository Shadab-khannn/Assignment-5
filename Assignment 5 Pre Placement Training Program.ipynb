{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0b29c-326c-4737-88ce-8777e53a9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the Naive Approach in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7d6d3-024b-48bb-aedf-cdae38335e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Naive Approach in machine learning, also known as the Naive Bayes algorithm, is a classification technique based on Bayes’ \n",
    "Theorem with an assumption of independence among predictors. In other words, it assumes that the presence or absence of a particular \n",
    "feature\n",
    "in a class is unrelated to the presence or absence of any other feature.\n",
    "\n",
    "Despite its simplicity and the naive assumption of independence, the Naive Approach can perform well in many real-world scenarios\n",
    ", particularly in text classification tasks such as spam filtering and sentiment analysis. \n",
    "It is easy to implement and can handle large feature spaces. It can be used for both binary and multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d4d5f-24e0-4f67-a770-90705d05bc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81754b-b3eb-41e9-8080-3f32656331ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain the assumptions of feature independence in the Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f9b72-5124-486f-86bf-c7190ed02316",
   "metadata": {},
   "outputs": [],
   "source": [
    "One of the main assumptions of the Naive Approach, also known as the Naive Bayes algorithm, is that the features are conditionally \n",
    "independent given the class label. \n",
    "This means that the presence or absence of a particular feature in a class is assumed to be unrelated to the presence or absence of \n",
    "any other feature.\n",
    "\n",
    "For example, lets say we have a dataset with two features, \"rain\" and \"umbrella\", and a binary class label \"wet\". \n",
    "The Naive Approach would assume that the probability of being wet given that it is raining is independent of whether or not someone \n",
    "is carrying an umbrella. In other words, the algorithm assumes that the presence of rain does not affect the probability of someone\n",
    "carrying an umbrella, and vice versa.\n",
    "\n",
    "This assumption is often referred to as the \"naive\" assumption because it is often not true in real-world scenarios. However,\n",
    "despite this simplification, the Naive Approach can still perform well in many classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e63a5-f666-4f7d-8c08-7c2c7029319a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eea564-b439-4b4f-a5fa-f157fed63cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How does the Naive Approach handle missing values in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c2eba9-1583-4ff3-be36-4fc5588a2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are different ways to handle missing values when training a Naive Bayes classifier. Some of the ways are:\n",
    "\n",
    "1. Omit records with any missing values\n",
    "2. Omit only the missing attributes\n",
    "3. Use an m-estimate of the probability which uses an equivalent sample size to calculate your probabilities\n",
    "4. Drop columns with missing values\n",
    "5. Remove corresponding terms from the classifier.\n",
    "\n",
    "Naive Bayes can handle missing data because attributes are handled separately by the algorithm at both model construction time and\n",
    "prediction time. As such, if a data instance has a missing value for an attribute, it can be ignored while preparing the model, and \n",
    "ignored when a probability is calculated for a class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa24f41-356b-4f70-a52a-6d42b3f48108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f78df-2e5e-4a09-bf46-fb351c5865f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What are the advantages and disadvantages of the Naive Approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643258f-5965-49b0-8aaa-5c609473dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Naive Approach, also known as the Naive Bayes algorithm, has several advantages.\n",
    "It is easy to implement and can be used for both binary and multi-class classification problems.\n",
    "It performs well in many real-world scenarios, particularly in text classification tasks such as spam filtering and sentiment analysis\n",
    "It can handle large feature spaces and is relatively fast to train and make predictions.\n",
    "\n",
    "However, the Naive Approach also has some disadvantages. One of its main assumptions is that the features are conditionally\n",
    "independent given the class label. This assumption often does not hold in real-world scenarios, which can lead to suboptimal \n",
    "performance.\n",
    "Another issue is the zero probability problem, where if a feature value is not present in the training data for a particular class, \n",
    "it can lead to zero probabilities and incorrect predictions1. Additionally, the Naive Approach is not suitable for regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc014921-2620-4715-a0c5-5389e7894ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e85dc-3891-4d7f-b81f-608f354dc492",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Can the Naive Approach be used for regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce0f65-7b70-4a7d-b441-4b71211e3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes is primarily used for classification tasks, but it can also be applied to regression problems by modeling the probability\n",
    "distribution of the target value with kernel density estimators.\n",
    "However, its performance in regression tasks may not be as good as in classification tasks due to the independence assumption made by\n",
    "the algorithm.\n",
    "\n",
    "In a study, it was shown that Naive Bayes applied to regression problems by discretizing the target value performed poorly.\n",
    "The study presented empirical evidence that isolated Naive Bayes’ independence assumption as the culprit for its poor performance in \n",
    "the regression setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90431d7b-59dc-4131-83ea-de410f8f62fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560bdb8-fbe9-456d-b7b3-23c125abf004",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. How do you handle categorical features in the Naive Approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c362782-3b28-4410-bb0d-4b009650ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "To handle categorical data in the Naive Approach, each category should be created as a feature with boolean values.\n",
    "This is done using one-hot encoding, where n-1 dummy variables or features are created for n categories and added to the data.\n",
    "If a particular category is associated with a row, it is assigned a value of 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fb131-4d14-49e4-8a63-3e6640b2859d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b42e7-45d9-4c1a-99b4-a38945ca3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What is Laplace smoothing and why is it used in the Naive Approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb4ae97-394a-43cd-8281-d73ee5bc6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "Laplace smoothing is a smoothing technique that can be used in Naïve Bayes classification. It is used to remove the problem of zero\n",
    "probability in the Naive Bayes Algorithm.\n",
    "The concept is to add a small positive value to each of the existing conditional probability values to avoid zero values in the\n",
    "probability model.\n",
    "\n",
    "This technique is also known as additive smoothing or Lidstone smoothing. It is a type of shrinkage estimator, as the resulting \n",
    "estimate will  be between the empirical probability (relative frequency), and the uniform probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a4229-0f1e-41b9-a9d3-d4eb66aa4e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0f2c2-3a06-46d2-b143-ae9d26d3652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. How do you choose the appropriate probability threshold in the Naive Approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72581f2-6fb9-4834-a959-99f1fa3e0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "The threshold is not a part of the Naive Bayes algorithm itself. A Naive Bayes algorithm will be able to say for a certain sample,\n",
    "that the probability of it being of one class is a certain percentage and of another class is another percentage. \n",
    "Then it’s up to you to interpret this as a classification in one class or another, which would be the case for a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6f7bc-66f8-43c2-ba8b-cbc82bac775a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c9ad2-5ab6-4740-a8b2-71de8a2bd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Give an example scenario where the Naive Approach can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad59a57-74a2-4883-84a6-7271b487e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "An example scenario where the Naive Approach can be applied is in text classification that includes a high-dimensional training\n",
    "dataset.\n",
    "Some popular examples of Naive Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.\n",
    "\n",
    "Another example is in weather prediction. Suppose we have a dataset of weather conditions and corresponding target variable \"Play\".\n",
    "So using this dataset we need to decide whether we should play or not on a particular day according to the weather conditions. \n",
    "To solve this problem, we need to follow these steps: Convert the given dataset into frequency tables, generate Likelihood table by\n",
    "finding the probabilities of given features, and use Bayes theorem to calculate the posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4821a-a88b-41a6-9fa4-f096de6d76d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9a0025d-9f5c-489f-b17e-35a1098c7582",
   "metadata": {},
   "source": [
    "### KNN:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cb3a9-b456-470b-8314-d512faed5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What is the K-Nearest Neighbors (KNN) algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3766481-7e50-4fb0-ad09-8e9259d280ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric supervised learning method used for classification and regression. \n",
    "It works by finding the k closest training examples in a data set to a new data point, and using the majority class among them \n",
    "to classify the new data point ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98717a90-720e-473d-bdd6-3e7b7b3c859d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1e8c1-8025-478e-b217-ec09e684cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. How does the KNN algorithm work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a017a8-f27d-4c67-b4b2-6b491f0bbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The KNN algorithm works by finding the k closest training examples in a data set to a new data point, and using the majority class \n",
    "among them to classify the new data point. \n",
    "The distance between the new data point and each training example is calculated using a distance metric such as Euclidean Distance, \n",
    "Manhattan Distance, or Minkowski Distance.\n",
    "The k training examples with the smallest distances to the new data point are then selected, and the majority class among them is used\n",
    "to classify the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef4f49-6857-4b14-a1c4-fab6465e5882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e760272-fe7d-4e09-8820-2d1cf535d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cdf299-754e-4069-93b4-401a0caa6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "The value of k in KNN is a hyperparameter that is chosen based on the problem at hand. \n",
    "A small value of k means that noise will have a higher influence on the result, while a large value makes it computationally\n",
    "expensive.\n",
    "A common approach to choosing the value of k is to use cross-validation, where different values of k are tried and the one that gives\n",
    "the best performance on a validation set is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2b934-d42d-41d5-8db7-377f22be3d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ae8b2-a688-448d-9103-49ba9e6945c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. What are the advantages and disadvantages of the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f3d24-2cdc-4049-9672-e043e9a42cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some advantages of the KNN algorithm are that it is simple and easy to understand, non-parametric, and no training is required. \n",
    "However, it also has some disadvantages such as being sensitive to outliers, computationally expensive, requires a good choice of k, \n",
    "and is limited to Euclidean distance.\n",
    "\n",
    "Advantages:\n",
    "- Simple and easy to understand\n",
    "- Non-parametric\n",
    "- No training required\n",
    "\n",
    "Disadvantages:\n",
    "- Sensitive to outliers\n",
    "- Computationally expensive\n",
    "- Requires a good choice of k\n",
    "- Limited to Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ae44a-5018-4938-a7ef-20bd90b0184c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3b6d2-1db0-4e7b-91b3-9166e9f50bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. How does the choice of distance metric affect the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96232323-daa5-454f-8737-47e3d915ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice of distance metric can affect the performance of KNN. The distance metric determines how the distance between the new data\n",
    "point and each training example is calculated. \n",
    "- Different distance metrics may give different results, and the best one to use depends on the problem at hand. \n",
    "- Common distance metrics used in KNN include Euclidean Distance, Manhattan Distance, and Minkowski Distance.\n",
    "- It is important to choose a distance metric that is appropriate for the data and the problem being solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21aa852-1eb4-4533-b41c-eda32e0f8fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324ded3-3978-4b85-9adb-0ecc92fe4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "15. Can KNN handle imbalanced datasets? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66b7fc-3680-4cf5-9d53-547b0a017307",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, KNN can handle imbalanced datasets using methods such as evolutionary optimized feature and class weights kNN (FCWkNN) or mixed\n",
    "weighted.\n",
    "KNN algorithm which assigns each sample of datasets an inverse proportion weight and combines it with the distance weight. \n",
    "These methods help to improve the performance of KNN on imbalanced datasets by taking into account the class, distribution around the \n",
    "neighborhood of the query instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8fe18-6323-4327-bf5b-9ba35aba004f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebdfa3-b636-4030-89b8-9f33dec16dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "16. How do you handle categorical features in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70b667-e542-488a-941f-e485b1ba533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical features can be handled in KNN by using techniques such as one-hot encoding or label encoding. \n",
    "One-hot encoding involves creating a new binary feature for each unique value of the categorical feature, where the value is 1 if \n",
    "the original feature has that value and 0 otherwise.\n",
    "Label encoding involves assigning a unique numerical value to each unique value of the categorical feature. \n",
    "These techniques allow the categorical features to be represented in a way that can be used by the distance metric in KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479215e3-9367-445b-91ed-b06988bdd1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958d8c5-f79c-45b0-aba7-947570a74696",
   "metadata": {},
   "outputs": [],
   "source": [
    "17. What are some techniques for improving the efficiency of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7ddf6-f367-4cc1-8b72-ce5c20153a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some techniques for improving the efficiency of KNN include reducing the dimensionality of the data using techniques such as Principa \n",
    "Component Analysis (PCA), using an appropriate data structure such as a KD-tree to store the data, and using an appropriate distance\n",
    "metric.\n",
    "Reducing the dimensionality of the data can help to speed up the distance calculations, while using an appropriate data structure can \n",
    "help to speed up the search for the k nearest neighbors. \n",
    "Using an appropriate distance metric can also help to improve the efficiency of KNN by reducing the number of distance calculations\n",
    "that need to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e1fc9-7d1a-4a2a-99d1-89866ea10915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27445701-f02c-4ccc-b7e8-58b6d9727afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "18. Give an example scenario where KNN can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9a718-1976-4d5d-827e-bf1d953d6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "An example scenario where KNN can be applied is in recommendation systems, where it can be used to find similar items or users based \n",
    "on their attributes or behavior.\n",
    "For example, in a movie recommendation system, KNN can be used to find users who have similar movie preferences to a given user, \n",
    "and recommend movies that those similar users have liked.\n",
    "This can help to provide personalized recommendations to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68f644-f226-4d94-9994-bf57c2d06ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b91b4138-cfe1-495e-8c0f-3d2261bafe77",
   "metadata": {},
   "source": [
    "### Clustering:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e7a08-c6df-42fb-a5d0-b22a0506b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "19. What is clustering in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fc823-a8f7-49a3-a37e-6b16cd5df5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering is a type of unsupervised learning in machine learning, where the goal is to group similar data points together into \n",
    "clusters.\n",
    "The algorithm tries to find patterns in the data and group them based on their similarity.\n",
    "Clustering is used in many applications such as customer segmentation, image segmentation, anomaly detection, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da959a-df18-48c3-81bd-491e0b892ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939074ac-6b25-404e-96c4-f71021e4ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "20. Explain the difference between hierarchical clustering and k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6ca33-9c96-4431-9a2f-af66846a757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K-means clustering and hierarchical clustering are two common methods of cluster analysis. The main differences between them are:\n",
    "\n",
    "- K-means clustering uses a pre-specified number of clusters and assigns records to each cluster to find mutually exclusive clusters\n",
    "  of spherical shape based on distance ¹. It requires advance knowledge of the number of clusters one wants to divide the data into.\n",
    "    \n",
    "- Hierarchical clustering , on the other hand, seeks to build a hierarchy of clusters without having a fixed number of clusters. \n",
    "  It can be either divisive or agglomerative ¹. In hierarchical clustering, one can stop at any number of clusters they find \n",
    "    appropriate by interpreting the dendrogram.\n",
    "\n",
    "In k-means clustering, since one starts with a random choice of clusters, the results produced by running the algorithm many times \n",
    "may differ.\n",
    "In contrast, results are reproducible in hierarchical clustering. \n",
    "K-means clustering is found to work well when the structure of the clusters is hyper-spherical (like a circle in 2D, sphere in 3D),\n",
    "while hierarchical clustering doesnt work as well as k-means when the shape of the clusters is hyper-spherical ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e14ed6-9bba-443f-8e1e-23702a05a400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785e987-510f-4216-bac9-f2f8a33518a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "21. How do you determine the optimal number of clusters in k-means clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948233c-cd33-48dd-a20f-21859b3b7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several methods to determine the optimal number of clusters in k-means clustering. One popular method is the Elbow Method.\n",
    "This method involves the following steps:\n",
    "    \n",
    "1. Compute k-means clustering for different values of k, for instance, by varying k from 1 to 10 clusters.\n",
    "2. For each k, calculate the total within-cluster sum of square (wss).\n",
    "3. Plot the curve of wss according to the number of clusters k.\n",
    "4. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.\n",
    "\n",
    "Another method is to maximize the average silhouette over a range of possible values for k ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8447e-22e0-4b8c-8634-bb753c5cdbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab76a89-5753-4048-a472-e32432c3cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "22. What are some common distance metrics used in clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df15785-34dc-4948-ad06-6a714ae9423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some common distance metrics used in clustering include Euclidean Distance, Manhattan Distance, Minkowski Distance, and Cosine\n",
    "Similarity. \n",
    "These distance metrics measure the similarity or dissimilarity between data points in different ways, and the best one to use depends\n",
    "on the problem at hand.\n",
    "Euclidean Distance measures the straight-line distance between two points, while Manhattan Distance measures the distance along the \n",
    "axes.\n",
    "Minkowski Distance is a generalization of Euclidean and Manhattan Distance, and Cosine Similarity measures the cosine of the angle \n",
    "between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3434292a-3e34-4e18-93fd-40e6b2b3a7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b6554-f5f9-43ef-aaa0-8c7e17ce86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "23. How do you handle categorical features in clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5a934-dfd8-4689-94bd-26f46355bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical features can be handled in clustering by using techniques such as one-hot encoding, label encoding, or by using algorithm \n",
    "specifically designed for categorical data, such as the k-modes or k-prototypes algorithms.\n",
    "One-hot encoding involves creating a new binary feature for each unique value of the categorical feature, where the value is 1 if the \n",
    "original feature has that value and 0 otherwise.\n",
    "Label encoding involves assigning a unique numerical value to each unique value of the categorical feature.\n",
    "These techniques allow the categorical features to be represented in a way that can be used by the distance metric in clustering \n",
    "algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a39026-d9da-431b-b4f4-0530205a1989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b5182-e5bb-4fe5-bfef-5d2e4f15bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "24. What are the advantages and disadvantages of hierarchical clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cc258-8a4f-49bb-979d-b022d5c85284",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hierarchical clustering is a type of clustering algorithm that creates a hierarchy of clusters by either merging smaller clusters into\n",
    "larger\n",
    "ones (agglomerative) or dividing larger clusters into smaller ones (divisive).\n",
    "Some advantages of hierarchical clustering include its ability to handle non-convex clusters and clusters of different sizes and \n",
    "densities,as well as its ability to handle missing data and noisy data³. It is also easy to understand and implement.\n",
    "\n",
    "However, hierarchical clustering also has some disadvantages. It can be computationally expensive for large datasets, and it involves\n",
    "making arbitrary decisions about the distance metric and linkage criteria used. \n",
    "It can also work poorly with mixed data types, and its main output, the dendrogram, is commonly misinterpreted.\n",
    "\n",
    "Advantages:\n",
    "- Can handle non-convex clusters and clusters of different sizes and densities\n",
    "- Can handle missing data and noisy data\n",
    "- Easy to understand and implement\n",
    "\n",
    "Disadvantages:\n",
    "- Computationally expensive for large datasets\n",
    "- Involves making arbitrary decisions about distance metric and linkage criteria\n",
    "- Can work poorly with mixed data types\n",
    "- Dendrogram output can be misinterpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de8d26-1421-44db-86a1-b0586c2c7fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82421e-4ecb-403f-818f-046c9d56eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "25. Explain the concept of silhouette score and its interpretation in clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6ad59-dea0-49f6-9c2d-4d633488fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The silhouette score is a measure of how well an object is classified in its cluster compared to other clusters. It ranges from -1 to\n",
    "+1,where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value,\n",
    "then the clustering configuration may have too many or too few clusters.\n",
    "\n",
    "The silhouette score is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each \n",
    "sample.\n",
    "The silhouette score for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that\n",
    "the sample is not a part of.\n",
    "\n",
    "A silhouette score close to 1 means that the data is appropriately clustered, while a score close to -1 means that the data point \n",
    "would be more appropriate if it was clustered in its neighboring cluster. A score near 0 means that the data point is on the border \n",
    "of two natural clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8e7d8-a9c0-410d-b304-9c98187939ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db5a1d-8ee4-489b-a81e-b676aa3f1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "26. Give an example scenario where clustering can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf1802-173b-4572-93ed-3ac54d9284df",
   "metadata": {},
   "outputs": [],
   "source": [
    "An example scenario where clustering can be applied is in customer segmentation for targeted marketing. \n",
    "A company may have a large customer base with diverse characteristics and behaviors.\n",
    "Clustering can be used to group customers into segments based on their similarities, such as demographics, purchase behavior, and\n",
    "preferences.\n",
    "This allows the company to tailor its marketing efforts to each segment, providing more relevant and personalized offers to its \n",
    "customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfb47e-03fe-4f6f-b459-8c71cbb2b1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e29b642c-5402-42a5-8427-61d933946b08",
   "metadata": {},
   "source": [
    "### Anomaly Detectio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3742f0f-8ff1-4360-a0e9-c90718023300",
   "metadata": {},
   "outputs": [],
   "source": [
    "27. What is anomaly detection in machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262969cf-6883-4578-b9c6-37915f4aec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anomaly detection is the technique of identifying rare events or observations which can raise suspicions by being statistically\n",
    "different from the rest of the observations 1. Such “anomalous” behavior typically translates to some kind of a problem like a credit\n",
    "card fraud,failing machine in a server, a cyber attack, etc ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538160d0-d189-40f2-89a8-01b83ed2343f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef0c10-38ac-4e6b-8a0f-581e1e1de839",
   "metadata": {},
   "outputs": [],
   "source": [
    "28. Explain the difference between supervised and unsupervised anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b8256-1e97-42fe-9bd5-7d9aa5dfb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are two main approaches to anomaly detection: supervised and unsupervised.\n",
    "\n",
    "1. Supervised anomaly detection requires a labeled dataset containing both normal and anomalous samples to construct a predictive \n",
    "   model to classify future data points.\n",
    "\n",
    "2.Unsupervised anomaly detection, on the other hand, does not require any training data. Instead, it assumes that only a small \n",
    "  percentage of data is anomalous and that any anomaly is statistically different from the normal samples. \n",
    "    Based on these assumptions, the data is then clustered using a similarity measure, and the data points which are far off from the\n",
    "    cluster are considered to be anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a8127-2ef1-4ea0-909f-aa1a66fb2562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4937ac-8a78-459f-8f9a-2760cc2b8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "29. What are some common techniques used for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98dba30-094a-4969-916f-fe5980b0876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some common techniques used for anomaly detection include:\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Support Vector Machine (SVM) learning\n",
    "- Neural Networks\n",
    "- Clustering-based methods\n",
    "- Statistical methods\n",
    "\n",
    "Each technique has its own strengths and weaknesses, and the choice of technique depends on the specific use case and the nature of \n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e87526-e656-40a5-a5f3-193d3aacb89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3bf42-89ab-4704-8691-270240ec99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "30. How does the One-Class SVM algorithm work for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29ea1b-dba0-4802-8b50-07025700d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "One-Class SVM is an algorithm used for anomaly detection. It works by finding the hyperplane that separates the training data from the\n",
    "origin with the maximum margin. The hyperplane is defined by a subset of training samples called support vectors.\n",
    "The algorithm then classifies new data points based on their distance from the hyperplane.\n",
    "If a new data point lies on the same side of the hyperplane as the training data, it is considered normal; otherwise, it is \n",
    "considered an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb8473-81a4-47f5-adb1-15dd32903292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b16c0f-c98c-4478-bd45-972e93d707f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "31. How do you choose the appropriate threshold for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5866ad-2d7b-42b7-ad83-777c53f16a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing an appropriate threshold for anomaly detection depends on the specific use case and the balance between false positives and\n",
    "false negatives.\n",
    "One approach is to use statistical methods such as z-scores or interquartile ranges to determine if a data point is an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9f87e-549b-43e1-9626-ed0417fd3f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a04ef-fa2d-4047-9cd1-252fa9613c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "32. How do you handle imbalanced datasets in anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0621cf25-cf02-4c3e-93c0-37ba7406dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling imbalanced datasets in anomaly detection can be challenging because anomalies are typically rare events.\n",
    "One approach is to use techniques such as oversampling or undersampling to balance the dataset before training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f9bf1-5fd3-4046-a2a8-d67565e00736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd9094-6da3-49c1-94fc-e887032428f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "33. Give an example scenario where anomaly detection can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c764506-1db2-4e1b-8d79-5a376a86da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n example scenario where anomaly detection can be applied is in credit card fraud detection. \n",
    "Anomaly detection algorithms can be used to identify unusual transactions that deviate from a user’s normal spending patterns,\n",
    "which may indicate fraudulent activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f14bf-1832-4bbd-8d32-1d4cadf62897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7d9fcf-6fe8-451c-bcf8-51a20443070a",
   "metadata": {},
   "source": [
    "### Dimension Reduction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780c37d-e16d-49bc-ac2e-cca3b19dac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "34. What is dimension reduction in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ea83b-efef-4a74-a5ed-c11424879c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dimension reduction in machine learning is the process of reducing the number of features in a dataset while retaining as much \n",
    "information as possible . It can be used to reduce the complexity of a model, improve the performance of a learning algorithm, or \n",
    "make it easier to visualize the data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3e86e-7dc7-4396-aab1-6f0b81b9a174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9a436-d4ea-4084-a5da-a4d82f04fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "35. Explain the difference between feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ed08a-4f6b-4497-abd2-35c22a555499",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are two main approaches to dimension reduction: feature selection and feature extraction .\n",
    "\n",
    "-Feature selection involves selecting a subset of the original features that are most relevant to the problem at hand.\n",
    "-The goal is to reduce the dimensionality of the dataset while retaining the most important features .\n",
    "-Feature extraction, on the other hand, involves creating new features by combining or transforming the original features .\n",
    "-The goal is to reduce the dimensionality of the dataset while retaining as much information as possible ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9cad6-7d82-4335-a6b2-b04600e96813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdb76a-2ef2-43b9-85bd-25e91da572a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "36. How does Principal Component Analysis (PCA) work for dimension reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b64209-3c35-486c-9a7b-78ea22f55015",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a popular technique for dimension reduction that falls under the category of feature extraction.\n",
    "It works by finding a new set of orthogonal axes (called principal components) that capture the most variance in the data. \n",
    "The first principal component captures the most variance, the second principal component captures the second most variance, and so on.\n",
    "By projecting the data onto a smaller number of principal components, we can reduce its dimensionality while retaining as much\n",
    "information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6044d-ad66-4e04-9dda-0946dc226a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f31e4ee-29d0-4b7f-82f3-9678423c7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "37. How do you choose the number of components in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b2441-b429-465e-a387-ec207fcdcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "The number of components in PCA is typically chosen based on the amount of variance explained by each component.\n",
    "One approach is to plot the cumulative variance explained by each component and choose a number of components that captures a high \n",
    "percentage (e.g., 95%) of the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97205eb-49f3-4210-a831-6378ccc7a83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0baf7-0b6f-4e68-8743-96c22209f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "38. What are some other dimension reduction techniques besides PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6a23a-ce64-40ec-8b11-77f85a1c13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some other dimension reduction techniques besides PCA include:\n",
    "- Linear Discriminant Analysis (LDA): a supervised technique that finds the linear combination of features that best separates two or\n",
    "  more classes.\n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE): a non-linear technique that is particularly effective at visualizing \n",
    "  high-dimensional data in two or three dimensions.\n",
    "- Autoencoders: a type of neural network that can learn a compressed representation of the input data.\n",
    "- Factor Analysis: a technique that models the covariance between observed variables in terms of a smaller number of latent variables.\n",
    "- Isomap: a non-linear technique that seeks to preserve the geodesic distances between all pairs of data points.\n",
    "\n",
    "Each technique has its own strengths and weaknesses, and the choice of technique depends on the specific use case and the nature of \n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e3c6b-2ed0-4445-bd50-d72a71e9515c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e1f9e-487d-4521-9ae9-fe89c9d3fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "39. Give an example scenario where dimension reduction can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a4b2e-3eb9-42ad-8876-3940b956ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "An example scenario where dimension reduction can be applied is in image classification. \n",
    "High-resolution images can have millions of pixels, making it challenging to train a machine learning model directly on the raw pixel\n",
    "data. \n",
    "By applying dimension reduction techniques such as PCA, we can reduce the dimensionality of the data while retaining its most \n",
    "important features, making it easier to train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc119cc-03c3-4bcb-b20d-e1015a7408ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ce1224d-9d5f-4395-a07b-06fb38803a0a",
   "metadata": {},
   "source": [
    "### Feature Selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7473b-614d-48b4-8f65-d8128c82bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "40. What is feature selection in machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065051e-ce60-414c-a1b0-ceac72e304b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection is the process of identifying and selecting a subset of the most relevant features from the original dataset to use\n",
    "as  inputs in a machine learning model.\n",
    "The goal is to reduce the dimensionality of the dataset while retaining the most important features ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dfd9c0-5f97-49ca-a8f8-b77c6ab2e21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8002c-4baa-43ca-9165-9b3a91ca2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "41. Explain the difference between filter, wrapper, and embedded methods of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c79b95-ffed-4859-93a2-20167d57c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are three main classes of feature selection algorithms: filter methods, wrapper methods, and embedded methods.\n",
    "\n",
    "- Filter methods rank the features based on their relevance to the target variable. They use statistical measures such as correlation\n",
    "  or mutual information to evaluate the relationship between each feature and the target variable.\n",
    "- Wrapper methods use the model performance as the criteria for selecting features. They search for the best subset of features by \n",
    "  evaluating the performance of a machine learning model trained on different subsets of features.\n",
    "- Embedded methods combine feature selection with the model training process 1. They select features by incorporating feature \n",
    "  selection as part of the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6155a-e160-4bcb-97f0-f8e248a480c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571f016-cc76-4e94-a457-c837ba4b9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "42. How does correlation-based feature selection work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99d3a5-de56-49e5-9307-7ad529fc7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "Correlation-based feature selection is a type of filter method that selects features based on their correlation with the target \n",
    "variable.\n",
    "It works by calculating the correlation between each feature and the target variable, and selecting the features with the highest\n",
    "correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997298d-5b0a-404e-8ec1-bd385ab232e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d9bf8-7f8c-4e63-87b2-d0eb5b8ea82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "43. How do you handle multicollinearity in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8ac79-445a-49a9-ab91-4bd7977044b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity refers to a situation where two or more features are highly correlated with each other. \n",
    "This can cause problems in feature selection because it can be difficult to determine which feature is more important. \n",
    "One approach to handling multicollinearity is to use techniques such as Principal Component Analysis (PCA) or Variance Inflation \n",
    "Factor (VIF) to identify and remove highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74755cc0-efbb-4e44-8af7-b73255a1fb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a68b0-ad0d-41d4-812e-c21b28d82942",
   "metadata": {},
   "outputs": [],
   "source": [
    "44. What are some common feature selection metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a15039-4c28-45a9-b81f-16ca56e74e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some common feature selection metrics include correlation, mutual information, chi-squared test, and F-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4abf4-d808-40e7-9b27-b9a32d6f14bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d9396-6339-4a66-9949-e486bfa21fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "45. Give an example scenario where feature selection can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493648f0-c79a-4250-a989-d02d0027e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "An example scenario where feature selection can be applied is in text classification. \n",
    "Text data often contains a large number of unique words, making it challenging to train a machine learning model directly on the raw\n",
    "word counts.\n",
    "By applying feature selection techniques such as mutual information or chi-squared test, we can identify and select the most important\n",
    "words,reducing the dimensionality of the data and making it easier to train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933bb733-720b-47c6-845d-fa521f8b8d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8071b69-2d61-43a5-9601-0f6c77f57a41",
   "metadata": {},
   "source": [
    "## Data Drift Detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a5021-46ec-4302-ab39-98296f548825",
   "metadata": {},
   "outputs": [],
   "source": [
    "46. What is data drift in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66401b-ffd3-4e7e-988e-bbe5cfef8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data drift is a phenomenon in machine learning where the statistical properties of the data used to train a machine learning model \n",
    "change over time. This can lead to a decrease in the accuracy of the model over time. \n",
    "Data drift can be caused by upstream process changes, such as a sensor being replaced that changes the units of measurement from \n",
    "inches to centimeters . Monitoring data drift helps detect these model performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07f392-be22-4fdb-a51d-f87ed082b801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c58912-1917-4c48-97ce-415949e04d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "47. Why is data drift detection important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea30684-c8ec-4de8-bda3-282172d5cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are two main types of data drift: concept drift and feature drift.\n",
    "\n",
    "-Concept drift, also known as model drift, occurs when the relationship between the input features and the target variable changes\n",
    "  over time.\n",
    "-Feature drift, also known as covariate shift, occurs when the distribution of the input features changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e9a3b-5181-4339-b84e-5aeb3439f46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629f25c-5138-44bb-a611-655009fc114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "48. Explain the difference between concept drift and feature drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508fef6-857d-43ec-8c7f-2e840544aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Detecting data drift is important because it can help identify when a machine learning model is no longer performing as well as it\n",
    "should be.\n",
    "By monitoring for data drift, one can take corrective action, such as retraining the model on new data or updating its parameters,\n",
    "to maintain its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81dff6-d7bf-4753-b7c2-1ed949d4d69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ceb11-c616-4091-9d34-d2ae88d258b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "49. What are some techniques used for detecting data drift?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61afdd9-54b0-45ba-bc0f-50ee4da75521",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several techniques used for detecting data drift, including statistical tests, density estimation, and classification-based\n",
    "methods. \n",
    "The choice of technique depends on the specific use case and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9769296-c1dd-4a9b-afd5-701c2d637edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb1912-8c1c-4366-bbff-a7a903acaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "50. How can you handle data drift in a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891dead2-21d6-4aa7-8f20-3d077e7667a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling data drift in a machine learning model typically involves retraining the model on new data that reflects the current \n",
    "distribution of the input features.\n",
    "This can be done by periodically collecting new training data and using it to update the model.\n",
    "In some cases, it may also be necessary to update the model architecture or hyperparameters to better handle the changing data \n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb40a60-7005-4b3d-ac12-5b928d0c68c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c97938d-d146-415c-9705-e926d57cb0b9",
   "metadata": {},
   "source": [
    "## Data Leakage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416e187-ac2a-440b-b37c-3428a506ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "51. What is data leakage in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef09e9-92ee-4a10-a342-306ce190f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data leakage is a problem in machine learning where information from outside the training dataset is used to create the model .\n",
    "This can cause the model to learn or know something that it otherwise would not know, and in turn invalidate the estimated performance\n",
    "of the model being constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1fdff-4047-4fed-a20d-17272074b57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca76d8-4bca-4f24-81fc-25698b580e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "52. Why is data leakage a concern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c60b8-7461-454b-833a-1c5a76bd86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data leakage is a concern because it can cause a machine learning model to appear more accurate than it actually is.\n",
    "This can lead to overly optimistic estimates of the model’s performance, and can result in poor performance when the model is deployed\n",
    "in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660a314-615b-4fc3-b04e-c4bef2b960ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b9015-ce89-467b-8a15-5454188248b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "53. Explain the difference between target leakage and train-test contamination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14a8be-bd36-4297-9e41-fb48c41f3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "here are two main types of data leakage: target leakage and train-test contamination.\n",
    "\n",
    "-Target leakage occurs when information about the target variable is inadvertently included in the training data. This can happen, \n",
    " for example, if a feature is derived from the target variable or if future information is used to create a feature.\n",
    "-Train-test contamination occurs when information from the test dataset is used to create or select features for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c97b5e-5bca-42e2-bfa8-12c1ea09e45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefcd4e-084d-4e5a-a0ca-4ae1236255f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "54. How can you identify and prevent data leakage in a machine learning pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b37842-7243-45c6-b338-8b88995a58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify and prevent data leakage in a machine learning pipeline, it is important to carefully review the data preprocessing and\n",
    "feature engineering steps to ensure that no information from outside the training dataset is being used.\n",
    "Some common sources of data leakage include:\n",
    "\n",
    "-Using future information to create features\n",
    "-Using information from the test dataset to create or select features for the training dataset\n",
    "-Using information about the target variable to create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40836782-0b80-4ea4-abf6-d4133fb1a5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d2781-8a74-4963-bc68-1ff0a97845b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "55. What are some common sources of data leakage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235714f-7b64-4778-89fe-d91592e40f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some common sources of data leakage include:\n",
    "- Using future information to create features: This can happen if a feature is derived from data that would not be available at the\n",
    "  time of prediction.\n",
    "- Using information from the test dataset to create or select features for the training dataset: This can happen if feature selection\n",
    "  or preprocessing is performed on the entire dataset rather than just on the training data.\n",
    "- Using information about the target variable to create features: This can happen if a feature is derived from the target variable,\n",
    "  either directly or indirectly.\n",
    "\n",
    "To prevent data leakage, it is important to carefully review the data preprocessing and feature engineering steps to ensure that no\n",
    "information from outside the training dataset is being used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1345be6-3fa9-4a90-bbcb-0a3c39f90048",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2218ba-5c7a-4755-997a-348e6a57daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "56. Give an example scenario where data leakage can occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f035e1c3-66b8-4dec-ade7-d0e388ab89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "An example scenario where data leakage can occur is in predicting patient outcomes in a hospital. If a feature such as \n",
    "“length of stay is derived from the discharge date, which occurs after the outcome has been determined, this could result in target \n",
    "leakage.\n",
    "Similarly, if feature selection is performed on the entire dataset rather than just on the training data, this could result in \n",
    "train-test contamination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90047f-7d5a-4873-930b-2daf4dd2aa13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b807869-7715-4359-bef2-90f48ea6ab8e",
   "metadata": {},
   "source": [
    "### Cross Validation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec1ccb-b849-4113-8489-d0f8780b3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "57. What is cross-validation in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87b9be-8243-49cc-a868-56a125a8e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross-validation is a technique used in machine learning to assess the performance of a model on an independent dataset. \n",
    "It involves dividing the dataset into k subsets, or folds, and then training the model on k-1 folds and evaluating its performance \n",
    "on the remaining fold. This process is repeated k times, with each fold serving as the test set once. \n",
    "The results of the k evaluations are then averaged to produce a single performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f75ce7-9f6a-4d30-b76c-23da1094fc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c03373-1359-4336-92cd-02c267cf91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "58. Why is cross-validation important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0c857-da51-4b0f-8095-70e94243cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross-validation is important because it provides a more robust estimate of the model’s performance on unseen data.\n",
    "By evaluating the model on multiple subsets of the data, cross-validation helps to reduce the variance of the performance estimate\n",
    "and provides a better indication of how well the model will generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef119441-082a-43ce-83cf-7081ded7cb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae014cf-7cf4-4cd4-8af8-3163b9428353",
   "metadata": {},
   "outputs": [],
   "source": [
    "59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1d045-3ad7-4b90-ae6d-858fb3fb38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several variations of cross-validation, including k-fold cross-validation and stratified k-fold cross-validation.\n",
    "\n",
    "1. K-fold cross-validation involves dividing the dataset into k subsets of roughly equal size. The model is then trained on k-1 folds\n",
    "   and evaluated on the remaining fold. This process is repeated k times, with each fold serving as the test set once.\n",
    "2. Stratified k-fold cross-validation is similar to k-fold cross-validation, but with one key difference: the folds are constructed \n",
    "  so that each fold has roughly the same proportion of samples from each class. This can be useful when dealing with imbalanced \n",
    "  datasets, where one class has many more samples than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5d554-4a59-4c60-9d46-3d14f3491a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bc1b8-090e-4f24-b502-321e726197cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "60. How do you interpret the cross-validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3e161-9723-4c69-a6a0-8fda19aa4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross-validation is a technique used in machine learning to evaluate the performance of a model on unseen data. \n",
    "It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, \n",
    "and training the model on the remaining folds.\n",
    "\n",
    "The results of each iteration are averaged, and it’s called the cross-validation accuracy. \n",
    "Cross-validation accuracy is used as a performance metric to compare the efficiency of different models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
